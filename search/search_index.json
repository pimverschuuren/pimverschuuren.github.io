{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction The goal of this page is to introduce the concepts of unfolding, the correcting of distortions on measured observables incurred by the measuring device. Outside particle physics this is also known as deconvolution or unsmearing and the mathematics can be found under the heading of inverse problems. Additionally, the tutorial shows how to install and use the latest version of RooUnfold , a ROOT -based unfolding framework. It currently implements five methods: Iterative bayes(as proposed by D'Agostini) Singular value decomposition (SVD; as proposed by H\u00f6cker and Kartvelishvili and implemented in TSVDUnfold) Iterative dynamically stabilized (IDS; as proposed by B. Malaescu) Bin-by-bin (simple correction factors) An interface to the TUnfold method developed by Stefan Schmitt Simple inversion of the response matrix without regularisation Gaussian Process unfolding (GP; as proposed by A. Bozson, G. Cowan and F. Spano) Poisson-based likelihood maximization RooUnfold was written by Tim Adye, Richard Claridge, Kerstin Tackmann, and Fergus Wilson. Tutorial Content The tutorial is subdivided into sections of which each constitutes a subsection for the theory and a subsection for the implementation with RooUnfold. The tutorial contains the following sections: Motivation for unfolding Mathematical formulation Building the response matrix Matrix inversion and bin-by-bin Regularization and the bias-variance trade-off Covariance matrix and bias MSE and coverage Closure and stress test Literature Here is a non-exhaustive list of supporting literature on unfolding in particle physics. Unfolding in particle physics: A window on solving inverse problems by Francesco Spano Data Unfolding Methods in High Energy Physics by Stefan Schmitt Comparison of unfolding methods using RooFitUnfold by Lydia Brenner, Pim Verschuuren, Rahul Balasubramanian, Carsten Burgard, Vincent Croft, Glen Cowan and Wouter Verkerke Statistical unfolding of elementary particle spectra: Empirical Bayes estimation and bias-corrected uncertainty quantification by Mikael Kuusela and Victor M. Panaretos Unfolding Methods in Particle Physics by Volker Blobel A survey of unfolding methods for particle physics by Glen Cowan","title":"Home"},{"location":"#introduction","text":"The goal of this page is to introduce the concepts of unfolding, the correcting of distortions on measured observables incurred by the measuring device. Outside particle physics this is also known as deconvolution or unsmearing and the mathematics can be found under the heading of inverse problems. Additionally, the tutorial shows how to install and use the latest version of RooUnfold , a ROOT -based unfolding framework. It currently implements five methods: Iterative bayes(as proposed by D'Agostini) Singular value decomposition (SVD; as proposed by H\u00f6cker and Kartvelishvili and implemented in TSVDUnfold) Iterative dynamically stabilized (IDS; as proposed by B. Malaescu) Bin-by-bin (simple correction factors) An interface to the TUnfold method developed by Stefan Schmitt Simple inversion of the response matrix without regularisation Gaussian Process unfolding (GP; as proposed by A. Bozson, G. Cowan and F. Spano) Poisson-based likelihood maximization RooUnfold was written by Tim Adye, Richard Claridge, Kerstin Tackmann, and Fergus Wilson.","title":"Introduction"},{"location":"#tutorial-content","text":"The tutorial is subdivided into sections of which each constitutes a subsection for the theory and a subsection for the implementation with RooUnfold. The tutorial contains the following sections: Motivation for unfolding Mathematical formulation Building the response matrix Matrix inversion and bin-by-bin Regularization and the bias-variance trade-off Covariance matrix and bias MSE and coverage Closure and stress test","title":"Tutorial Content"},{"location":"#literature","text":"Here is a non-exhaustive list of supporting literature on unfolding in particle physics. Unfolding in particle physics: A window on solving inverse problems by Francesco Spano Data Unfolding Methods in High Energy Physics by Stefan Schmitt Comparison of unfolding methods using RooFitUnfold by Lydia Brenner, Pim Verschuuren, Rahul Balasubramanian, Carsten Burgard, Vincent Croft, Glen Cowan and Wouter Verkerke Statistical unfolding of elementary particle spectra: Empirical Bayes estimation and bias-corrected uncertainty quantification by Mikael Kuusela and Victor M. Panaretos Unfolding Methods in Particle Physics by Volker Blobel A survey of unfolding methods for particle physics by Glen Cowan","title":"Literature"},{"location":"setup/","text":"ROOT RooUnfold is written with the use of ROOT-libraries so before building RooUnfold one needs to have a recent version of ROOT setup correctly. The fully tested and recommended ROOT version is 6.20.06. If your platform mounts CVMFS (as, for example, CERN LXPLUS does), a pre-build ROOT can be setup with the command source /cvmfs/sft.cern.ch/lcg/app/releases/ROOT/6.20.06/ platform /bin/thisroot.sh Otherwise, one can easily follow the instructions on their install webpage . After setup check if the $ROOTSYS environment variable points to the top-level ROOT directory, $ROOTSYS/bin is in your $PATH, and $ROOTSYS/lib should be in your library path ($LD_LIBRARY_PATH on most Unix systems). RooUnfold After setting up ROOT, one can clone and build the RooUnfold library. Start by checking out the RooUnfold repository. git clone https://gitlab.cern.ch/cburgard/RooUnfold.git cd RooUnfold One can build the library with GNU make or with cmake mkdir build cd build cmake .. make -j4 cd .. source build/setup.sh There are several ROOT-macro's and python scripts in the subdirectory examples and executables in the subdirectory build that can be used to test the setup. RooUnfold Tutorial Code The tutorial is divided into sections with each section a piece of example code. The easiest way to run this code is to put it in a ROOT-macro void exampleMacro(){ // Put your code here } and then run it in the terminal with root exampleMacro.cxx Note that the name of the file and the main function in the file are the same. An error will be thrown if it is not. Docker","title":"Setup"},{"location":"setup/#root","text":"RooUnfold is written with the use of ROOT-libraries so before building RooUnfold one needs to have a recent version of ROOT setup correctly. The fully tested and recommended ROOT version is 6.20.06. If your platform mounts CVMFS (as, for example, CERN LXPLUS does), a pre-build ROOT can be setup with the command source /cvmfs/sft.cern.ch/lcg/app/releases/ROOT/6.20.06/ platform /bin/thisroot.sh Otherwise, one can easily follow the instructions on their install webpage . After setup check if the $ROOTSYS environment variable points to the top-level ROOT directory, $ROOTSYS/bin is in your $PATH, and $ROOTSYS/lib should be in your library path ($LD_LIBRARY_PATH on most Unix systems).","title":"ROOT"},{"location":"setup/#roounfold","text":"After setting up ROOT, one can clone and build the RooUnfold library. Start by checking out the RooUnfold repository. git clone https://gitlab.cern.ch/cburgard/RooUnfold.git cd RooUnfold One can build the library with GNU make or with cmake mkdir build cd build cmake .. make -j4 cd .. source build/setup.sh There are several ROOT-macro's and python scripts in the subdirectory examples and executables in the subdirectory build that can be used to test the setup.","title":"RooUnfold"},{"location":"setup/#roounfold-tutorial-code","text":"The tutorial is divided into sections with each section a piece of example code. The easiest way to run this code is to put it in a ROOT-macro void exampleMacro(){ // Put your code here } and then run it in the terminal with root exampleMacro.cxx Note that the name of the file and the main function in the file are the same. An error will be thrown if it is not.","title":"RooUnfold Tutorial Code"},{"location":"setup/#docker","text":"","title":"Docker"},{"location":"tutorial/","text":"Motivation for unfolding In any experimental setup the measuring device will have imperfections such as finite resolution, limited acceptance or detection efficiency. These imperfections will result in a measurement that is distorted from nature. Correcting for these distortions is known as unfolding. The general motivation for unfolding is to get results that are not measurement device dependent and can be compared with the results of other experiments. It should be noted that it is not always necessary to unfold. In case one wants to compare a measurement to a theoretical prediction it is often easier to simulate the detector effects on the theoretical prediction. However, for a measurement to retain its value over time and be comparable with possible future theoretical predictions the detector response, in one form or another, should be preserved as well. This is often impractical. Unfolding provides a result which can be directly compared with those of other experiments and theoretical predictions. Mathematical formulation Lets assume we have some measurable quantity, eg. the transverse momentum of a particle, respresented by the random variable x with probability density function g(x) . Each measured value x has a true value for a random variable y with probability density function f(y) . These two pdfs are related by a convolution g(x) = \\int R(x|y) f(y) dy where R(x|y) is the response function. In particle physics, the measured quantities are binned into histograms so we will focus on the discretized version of the problem. The above convolution changes to the linear equation \\vec{\\nu} = R\\vec{\\mu} with \\vec{\\nu} being the N -vector of expectation values for the bins of observed variable x and \\vec{\\mu} being the M -vector of expectation values for the histogram of y . The histogram of the actual observed data is given by the N -vector \\vec{n} . The response matrix R is a N \\times M matrix and has an interpretation of a conditional probability. R_{ij} = P(\\mbox{observed in bin }i | \\mbox{true value in bin }j) = \\frac{P(\\mbox{observed in bin }i\\mbox{ and } \\mbox{true value in bin }j)}{P(\\mbox{true value in bin }j)} The observed data \\vec{n} , their expectation values \\vec{\\nu} , the response matrix R and the expectation values of the true histogram \\vec{\\mu} are finally related by E[\\vec{n}] = \\vec{\\nu} = R\\vec{\\mu} Building the response matrix The first step of unfolding is building the response matrix. Here we assume that the analyst has chosen an observable to unfold, a Monte Carlo simulation to fill the expected truth distribution \\vec{\\mu} and a detector simulation to fill the expected reconstructed distribution \\vec{\\nu} . The result is a table of events with a true and reconstructed value of the chosen observable. Lets assume we filled a 2D histogram A with the reconstructed values on the x-axis and the true values on the y-axis. Recall that R_{ij} = \\frac{P(\\mbox{observed in bin }i\\mbox{ and } \\mbox{true value in bin }j)}{P(\\mbox{true value in bin }j)} These probabilities can be approximated in the limit of many events by P(\\mbox{observed in bin }i\\mbox{ and } \\mbox{true value in bin }j) = \\frac{\\mbox{Number of reconstructed events in bin }i\\mbox{ and true events in } j}{\\mbox{Total number of events}} = \\frac{A_{ij}}{\\sum_{i}^{N}\\sum_{j}^{M}A_{ij}} and P(\\mbox{true value in bin }j) = \\frac{\\mbox{Number of true events in bin }j}{\\mbox{Total number of events}} = \\frac{\\sum_{i}^{N}A_{ij}}{\\sum_{i}^{N}\\sum_{j}^{M}A_{ij}} The response matrix element can therefore be approximated by R_{ij} = \\frac{A_{ij}}{\\sum_{i}^{N}A_{ij}} One should note that this approximation will hold for diagonal matrix elements and the ones close to the diagonal i.e. the elements corresponding to heavily populated bins. One can change the binning to increase the bin counts or apply some sort of smoothing to ensure the validity of the response matrix. Efficiency In the above we assumed that for every true value there is a corresponding recontructed value. However, in reality the detector will have an efficiency resulting in a fraction of the true events not being reconstructed. Because one can only use events with both a true and reconstructed value to fill the histogram A the sum \\sum_{i}^{N}A_{ij} will no longer be equal to the total number of generated events. Instead one needs to fill a 1D histogram \\vec{\\mu} with all the true values and use that to calculate the response matrix. R_{ij} = \\frac{A_{ij}}{\\mu_{j}} The efficiency i.e. the number of events from truth bin j that are reconstructed in any bin is then \\epsilon_{j} = \\sum_{i}^{N}R_{ij} Code Example Lets start with some code that simulates event generation. In this example we use RooFit classes to sample values from a falling distribution. // A variable for the truth distribution. RooRealVar xtrue(\"xtrue\",\"xtrue\",-5,5); // A parameter of the falling distribution defining the shape. RooRealVar lambda(\"lambda\",\"lambda\",-0.00035); // The falling distribution. RooExponential truthPDF(\"truthPDF\",\"truthPDF\", xtrue, lambda); Define some simple variable dependent smearing and efficiency function. Double_t Smear(Double_t xtrue){ // Simulate efficiency. Double_t xeff= 0.3 + (1.0-0.3)/20*(xt+10.0); Double_t x= gRandom- Rndm(); if (x xeff) return -1; // Simulate smearing. Double_t xsmear= gRandom- Gaus(0,0.05*xtrue); return xt+xsmear; } Define the binning of the truth and reconstructed distribution. Double_t reco_binning[26] = {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,20,25,30,35,40,50,60}; Double_t truth_binning[13] = {0,2,4,6,8,10,12,14,18,25,35,45,60}; TH1D* truthHist = new TH1D(\"truthHist\",\"truthHist\",12,truth_binning); TH1D* recoHist = new TH1D(\"recoHist\",\"recoHist\",25,reco_binning); TH2D* responseHist = new TH2D(\"response\",\"response\", 25, reco_binning, 12, truth_binning); Define the histograms for some data to unfold and a truth distribution to compare the unfolding results with. TH1D* truthTestHist = new TH1D(\"truthTestHist\",\"truthTestHist\",12,truth_binning); TH1D* dataHist = new TH1D(\"dataHist\",\"dataHist\",25,reco_binning); Loop over the generated events, smear them and use them to fill the histograms. // Realistically, there are more MC events than data events available. Int_t n_MC_events = 500000; Int_t n_data_events = 10000; // Calculate the lumi weight with which the MC will be scaled down. Double_t lumiScale = (double)n_data_events/n_MC_events; // Generate the events; RooDataSet* MCEvents = truthPDF.generate(xtrue,n_MC_events); RooDataSet* dataEvents = truthPDF.generate(xtrue,n_data_events); // Loop over both datasets and fill the histograms. for (int i = 0; i n_MC_events; i++){ Double_t xtrue_val = ((RooRealVar*)(MCEvents- get(i))- find(xtrue.GetName()))- getVal(); Double_t xreco_val = Smear(xtrue_val); truthHist- Fill(xtrue_val, lumiScale); if (xreco_val 0){ recoHist- Fill(xreco_val, lumiScale); responseHist- Fill(xreco_val, xtrue_val, lumiScale); } } for (int i = 0; i n_data_events; i++){ Double_t xtrue_val = ((RooRealVar*)(dataEvents- get(i))- find(xtrue.GetName()))- getVal(); Double_t xreco_val = Smear(xtrue_val); truthTestHist- Fill(xtrue_val); if (xreco_val 0){ dataHist- Fill(xreco_val); } } Create a factory object that will build the response matrix and book keep all of the histograms. RooUnfoldSpec spec(\"unfold\",\"unfold\",truthHist,\"obs_truth\",recoHist,\"obs_reco\",responseHist,NULL,dataHist,false,0.0005); Input distributions Unnormalized response matrix Matrix inversion and bin-by-bin In this section we will discuss two unfolding methods i.e. two approaches for building estimators for the true distribution \\vec{\\mu} . These methods can be used in certain scenarios but often will show to give unsatisfactory results. Matrix inversion Suppose the response matrix can be inverted. In that case, one can write \\vec{\\mu} = R^{-1}\\vec{\\nu} A reasonable choice of estimators for \\vec{\\mu} would then be \\hat{\\vec{\\mu}} = R^{-1}\\vec{n} One can show that this is equivalent to a maximum likelihood(ML) solution under the assumption that the elements of the data vector \\vec{n} are independent and Poisson distributed around mean \\vec{\\nu} . In case the off-diagonal elements of the response matrix are large the estimator \\hat{\\vec{\\mu}} will have large variances and strong negative correlations. Show response matrix, truth, reconstructed, data and largely fluctuating estimator of the truth histogram. The small fluctuations in \\vec{n} are \"seen by R^{-1} \" as a fine structure that is enlarged in the estimators. One should note though that the ML/matrix inversion estimators are unbiased and have the smallest possible variance. We will show later that in order to reduce the variance one needs to introduce some bias by regularization. Bin-by-bin An alternative method with much smaller variances is applying bin-by-bin correction factors. \\hat{\\mu}_{i} = C_{i}n_{i} with C_{i} = A_{ii} This method is effectively the same as applying the matrix inversion method with all the off-diagonal elements set to zero. This will reduce the variance substantially but also introduce some bias. Both the matrix inversion and the bin-by-bin methods can be reasonable solutions in case of very small off-diagonal response matrix elements. However, often one needs to apply some form of regularization that constrains \\vec{\\mu} -space in which solutions are deemed reasonable. Code Example Lets use the generated data to unfold with both matrix inversion and bin-by-bin. // Instantiate a RooUnfoldFunc RooUnfoldFunc* unfoldFuncInvert = (RooUnfoldFunc*)spec.makeFunc(RooUnfolding::kInvert); RooUnfoldFunc* unfoldFuncBBB = (RooUnfoldFunc*)spec.makeFunc(RooUnfolding::kBinByBin); // Instantiate a RooUnfold object with RooFitHist as template type. RooUnfoldT RooUnfolding::RooFitHist,RooUnfolding::RooFitHist * unfoldInvert = const_cast RooUnfoldT RooUnfolding::RooFitHist,RooUnfolding::RooFitHist * (unfoldFuncInvert- unfolding()); RooUnfoldT RooUnfolding::RooFitHist,RooUnfolding::RooFitHist * unfoldBBB = const_cast RooUnfoldT RooUnfolding::RooFitHist,RooUnfolding::RooFitHist * (unfoldFuncBBB- unfolding()); // Unfold and get the unfolded distribution and errors. TVectorD invertUnfolded = unfoldInvert- Vunfold(); TVectorD invertErrors = unfoldInvert- EunfoldV(RooUnfolding::kErrors); TVectorD BBBUnfolded = unfoldBBB- Vunfold(); TVectorD BBBErrors = unfoldBBB- EunfoldV(RooUnfolding::kErrors); Distribution unfolded with matrix inversion Distribution unfolded with Bin-by-bin Regularization and bias-variance trade-off Recall that the observed data \\vec{n} is independent and Poisson distributed around \\vec{\\nu} . We can therefore construct the following log-likelihood. \\log L(\\vec{\\mu}) = \\sum_{i}^{N}(n_{i}\\log \\nu_{i} - \\nu_{i}) \\mbox{ with } \\nu_{i} = \\sum_{j}^{M}R_{ij}\\mu_{j} The solution that maximizes this likelihood is \\vec{n} = \\vec{\\nu} i.e. the matrix inversion solution. This is an example of overfitting and can be avoided by introducing some constraint on the likelihood maximization in the form of a regularization function. \\phi(\\vec{\\mu}) = \\log L(\\vec{\\mu}) + \\tau S(\\vec{\\mu}) Here the regularization function introduces some form of prior knowledge on what the solution should look like i.e. it introduces bias but reduces the variance of the estimator \\hat{\\vec{\\mu}} . This is also known as the bias-variance trade-off. Both variance and bias are sources of error on the predictive model that is the truth estimator \\hat{\\vec{\\mu}} . The regularization parameter \\tau determines where this bias-variance trade-off will lie. We will discuss later how to choose the regularization paremeter. The choice of S(\\vec{\\mu}) depends on the problem but often is chosen to impose some amount of smoothness on the estimator \\vec{\\mu} eg. with Tikhonov or entropy-based regularization. In the following section we will discuss some unfolding algorithms and their form of regularization that are commonly used in particle physics. Tikhonov regularization A very common choice of S(\\vec{\\mu}) is one based on the derivatives of the true distribution. Tikhonov regularization constructs a regularization function by taking the square of the k -th derivative of the true distribution. For k=2 and with finite differences the regularization function is S(\\vec{\\mu}) = - \\sum_{i=1}^{M-2}(-\\mu_{i} + 2\\mu_{i+1} - \\mu_{i+2})^{2} This function quantifies the amount of curvature the true distribution has. Distributions such as the ML solution have very large second derivatives and are therefore penalized heavily. A. N. Tikhonov and V. Y. Arsenin. Solutions of Ill-posed problems. W.H. Winston, 1977. Code Example RooUnfold has two Tikhonov regularized unfolding implementations: RooUnfoldSVD and RooUnfoldTUnfold. RooUnfoldSVD will be discussed seperately in an upcoming section. TUnfold implements a Gaussian likelihood and calculates the unfolded distribution with a closed form. // Instantiate a RooUnfoldFunc. Pass the regularization parameter tau=0.001. RooUnfoldFunc* unfoldFuncTUnfold = (RooUnfoldFunc*)spec.makeFunc(RooUnfolding::kTUnfold,0.001); // Instantiate a RooUnfold object with RooFitHist as template type. RooUnfoldT RooUnfolding::RooFitHist,RooUnfolding::RooFitHist * unfoldTUnfold = const_cast RooUnfoldT RooUnfolding::RooFitHist,RooUnfolding::RooFitHist * (unfoldFuncTUnfold- unfolding()); // Unfold and get the unfolded distribution and errors. TVectorD tunfoldUnfolded = unfoldTUnfold- Vunfold(); TVectorD tunfoldErrors = unfoldTUnfold- EunfoldV(RooUnfolding::kErrors); Gaussian likelihood and Tikhonov regularization with \\tau=10 and \\tau=0.001 Gaussian likelihood and Tikhonov regularization with \\tau=0.0005 and \\tau=0.000001 Richardson-Lucy (Iterative Bayes) The Richardson-Lucy algorithm, or iterative bayes unfolding as described by D\u2019Agostini in, is an unfolding procedure that updates the estimator \\vec{\\mu} with each iteration. The estimator is defined as \\hat{\\mu}_{j}^{r+1} = \\hat{\\mu}_{j}^{r}\\sum_{i}^{N}\\frac{R_{ij}n_{i}}{\\sum_{k}^{M}R_{ik}\\hat{\\mu}_{j}^{r}} for the j -th bin and r+1 iterations. The analyst needs to supply an initial truth distribution \\hat{\\vec{\\mu}}^{0} . The number of iterations determines the amount of regularization where few iterations will result in an estimator with small variance but will be biased towards this initial truth distribution. The bias-variance balance can be tipped in the other direction by increasing the number of iterations. In the limit of many iterations it is emperically shown that the estimator will converge to the ML estimator. W. H. Richardson. Bayesian-based iterative method of image restoration. J. Opt. Soc. Am., 62(1):55\u201359, Jan 1972. L. B. Lucy. An iterative technique for the rectification of observed distributions. 79:745, Jun 1974. G. D\u2019Agostini. A multidimensional unfolding method based on bayes theorem. 1995. Nucl. Instrum. Meth. A 362 (1995) 487. Code Example // Instantiate a RooUnfoldFunc. Pass the regularization parameter r=20. RooUnfoldFunc* unfoldFuncBayes = (RooUnfoldFunc*)spec.makeFunc(RooUnfolding::kBayes,20); // Instantiate a RooUnfold object with RooFitHist as template type. RooUnfoldT RooUnfolding::RooFitHist,RooUnfolding::RooFitHist * unfoldBayes = const_cast RooUnfoldT RooUnfolding::RooFitHist,RooUnfolding::RooFitHist * (unfoldFuncBayes- unfolding()); // Unfold and get the unfolded distribution and errors. TVectorD bayesUnfolded = unfoldBayes- Vunfold(); TVectorD bayesErrors = unfoldBayes- EunfoldV(RooUnfolding::kErrors); Iterative bayes with r=1 and r=20 Iterative bayes with r=100 and r=2000 Singular Value Decomposition (SVD) Singular Value Decomposition is an unfolding method that constructs a likelihood with the data \\vec{n} Gaussian distributed. The mean is taken to be the expected bin count \\vec{\\nu} and a measured covariance V_{ij} . The regularization function is the discretized Tikhonov second derivative regularization. The constrained log-likelihood can be written down in matrix form as \\phi(\\vec{\\mu}) = -\\frac{1}{2}(R\\vec{\\mu} - \\vec{n})V^{-1}(R\\vec{\\mu} - \\vec{n})^{T} + \\tau (C\\vec{\\mu})^{T} (C\\vec{\\mu}) SVD unfolding sets this expression to zero and solves for \\vec{\\mu} with the use of singular value decomposition. It also uses singular value decomposition to determine which of the data bins contribute the most to large fluctuations in the unfolded distribution and use that information to choose \\tau . The user actually choses an integer k between 0 and the number of truth bins N which then is translated into a value for \\tau that supresses the N-k data bins that contribute the most to the unwanted large fluctuations in the unfolded distribution. So k=0 will result in a heavily regularized distribution and k=N will return the ML solution. Hocker and Kartvelishvili. Svd-based unfolding: implementation and experience. PHYSTAT 2011, 2011. K. Tackmann. Svd approach to data unfolding. 1996. Nucl. Instrum. Meth. A 372 (1996) 469. Code Example // Instantiate a RooUnfoldFunc. Pass the regularization parameter k=8. RooUnfoldFunc* unfoldFuncSvd = (RooUnfoldFunc*)spec.makeFunc(RooUnfolding::kSVD,8); // Instantiate a RooUnfold object with RooFitHist as template type. RooUnfoldT RooUnfolding::RooFitHist,RooUnfolding::RooFitHist * unfoldSvd = const_cast RooUnfoldT RooUnfolding::RooFitHist,RooUnfolding::RooFitHist * (unfoldFuncSvd- unfolding()); // Unfold and get the unfolded distribution and errors. TVectorD svdUnfolded = unfoldSvd- Vunfold(); TVectorD svdErrors = unfoldSvd- EunfoldV(RooUnfolding::kErrors); SVD with k=2 and k=8 SVD with k=10 and k=12 Iterative Dynamically Stabilized (IDS) Iterative Dynamically Stabilized unfolding is an unfolding procedure that starts from an initial unfolded distribution very biased towards a normalized truth distribution. It then tries to improve the result by improving the response matrix with each iteration. The estimators are defined as \\hat{\\mu_{i}} = \\mu_{i}^{MC}C + \\Delta\\beta_{i}^{true} + \\sum_{j}^{M}\\big(f(\\Delta n_{i}, \\sigma_{\\Delta n_{i}}, \\lambda)R_{ji}\\Delta n_{i} - (1 - f(\\Delta n_{i}, \\sigma_{\\Delta n_{i}}, \\lambda))\\Delta n_{i}\\delta_{ji}\\big) with \\mu_{i}^{MC} being some initial truth distribution, C a normalization constant that corrects for differences in MC and data, \\Delta\\beta_{i}^{true} the expected number of truth events associated with fluctations in background subtraction, \\Delta n_{i} = n_{i} - \\Delta\\beta_{i}^{reco} - C\\nu_{i} , \\sigma_{\\Delta n_{i}} the uncertainty on the measured data n_{i} , R_{ji} the response matrix, f(\\Delta n_{i}, \\sigma_{\\Delta n_{i}}, \\lambda) a number between 0 and 1 and \\lambda that regulates this fraction. This is used for an initial unfolded distribution and then improved by changing the response matrix R_{ij} with each iteration. B. Malaescu. An iterative, dynamically stabilized method of data unfolding. arXiv:0907.3791. Code Example // Instantiate a RooUnfoldFunc RooUnfoldFunc* unfoldFuncIds = (RooUnfoldFunc*)spec.makeFunc(RooUnfolding::kIDS,10); // Instantiate a RooUnfold object with RooFitHist as template type. RooUnfoldT RooUnfolding::RooFitHist,RooUnfolding::RooFitHist * unfoldIds = const_cast RooUnfoldT RooUnfolding::RooFitHist,RooUnfolding::RooFitHist * (unfoldFuncIds- unfolding()); // Unfold and get the unfolded distribution and errors. TVectorD idsUnfolded = unfoldIds- Vunfold(); TVectorD idsErrors = unfoldIds- EunfoldV(RooUnfolding::kErrors); IDS with \\lambda=2 and \\lambda=20 IDS with \\lambda=100 and \\lambda=200 Gaussian Processes (GP) Gaussian Processes uses Bayesian regression to define an estimator as the mode of the posterior distribution. The posterior distribution is constructed with Bayes' theorem from a Gaussian likelihood and a Gaussian Process(GP) prior. Regularisation is introduced via the kernel function of the GP. Depending on the prior knowledge one has on the truth distribution different choices of kernels are possible. The amount of regularisation is determined by the parameters of the kernel which can be varied until unfolding results show satisfactory. It is also possible to choose these parameters based on the maximization of the likelihood marginalized over the truth distribution. The current implementation of RooUnfold has two kernel functions available: Radial kernel (Parameters: (A, l) ) k(x, x') = A \\mbox{exp}\\Bigg( \\frac{-(x - x')^{2}}{2l^{2}}\\Bigg) Gibbs kernel (Parameters: (A,b,c) ) k(x, x') = A \\sqrt{\\frac{2l(x)l(x')}{l^{2}(x) + l^{2}(x')}}\\mbox{exp}\\Bigg( \\frac{-(x - x')^{2}}{l^{2}(x) - l^{2}(x')}\\Bigg) with l(x) = bx + c and x and x' denoting two bin centers. A. Bozson, G. Cowan, and F. Spano. Unfolding with Gaussian Processes. 11 2018. Code Example // Instantiate a RooUnfoldFunc. The regularization parameter determines the kernel. // 1 = Radial kernel and 2 = Gibbs kernel. RooUnfoldFunc* unfoldFuncGP = (RooUnfoldFunc*)spec.makeFunc(RooUnfolding::kGP,2); // Instantiate a RooUnfold object with RooFitHist as template type. RooUnfoldT RooUnfolding::RooFitHist,RooUnfolding::RooFitHist * unfoldGP = const_cast RooUnfoldT RooUnfolding::RooFitHist,RooUnfolding::RooFitHist * (unfoldFuncGP- unfolding()); // The unfolding automatically maximizes a marginalized likelihood to find // the optimal kernel parameters. However, it is possible to tune the kernel parameters // and therefore the regularization by hand. Add the parameters in the order of (A, L) and // (A, b, c). std::vector double kernelParm; kernelParm.push_back(22); kernelParm.push_back(0.00001); kernelParm.push_back(9.9); ((RooUnfoldGPT RooUnfolding::RooFitHist,RooUnfolding::RooFitHist *)unfold)- SetKernelParm(kernelParm); // Unfold and get the unfolded distribution and errors. TVectorD gpUnfolded = unfoldGP- Vunfold(); TVectorD gpErrors = unfoldGP- EunfoldV(RooUnfolding::kErrors); GP with a Gibbs kernel with maximized marginal likelihood optimized kernel parameters and the above chosen kernel parameters. Covariance matrix and bias As mentioned before, the estimators \\hat{\\vec{\\mu}} have two sources of error, variance and bias. These are the quantities that the analyst should first look at when assessing their unfolding algorithm. Also, one will see in upcoming sections that these quantities and derivations thereof can be used to compare unfolding algorithms and tune the regularization. Additionally, one needs the covariance matrix if one wants to use the unfolded distribution in a fit. Covariance Lets recall the definition of the covariance matrix. \\mbox{Cov}(\\hat{\\mu}_{i}, \\hat{\\mu}_{j}) = E[(\\hat{\\mu}_{i} - E[\\hat{\\mu}_{i}])(\\hat{\\mu}_{j} - E[\\hat{\\mu}_{j}])] All of the above mentioned algorithms have their own approach for estimating the covariance matrix. However, it is also possible to estimate the covariance matrix with the use of toy data. Generating toy data or \"throwing toys\" is sampling pseudo-data \\vec{n}_{toy} from probability distributions of the observed data \\vec{n} . In particle physics, a very common choice is Poisson distributions with mean \\vec{\\nu} . These pseudo-data vectors can then be unfolded with the chosen unfolding algorithm and regularization parameter. In the limit of many toys these unfolded distributions can be used to calculate the covariance with the sample covariance \\mbox{C}(\\hat{\\mu}_{i}, \\hat{\\mu}_{j}) = \\frac{1}{K-1}\\sum_{k}^{K}\\Big(\\big((\\hat{\\mu}_{i})_{k} - \\frac{1}{K}\\sum_{k}^{K}(\\hat{\\mu}_{i})_{k}\\big)\\big((\\hat{\\mu}_{j})_{k} - \\frac{1}{K}\\sum_{k}^{K}(\\hat{\\mu}_{j})_{k}\\big)\\Big) with (\\hat{\\mu}_{i})_{k} being the i -th unfolded bin of the k -th toy for K toy datasets. Code Example // Calculate the covariance matrix with the unfolding algorithm specific method. TMatrixD cov = unfold- Eunfold(RooUnfolding::kCovariance); // Set the number of toys. unfold- SetNToys(1000000); // Calculate the covariance matrix with toys. TMatrixD covtoys = unfold- Eunfold(RooUnfolding::kCovToys); Covariance normalized to correlation matrix for matrix inversion without and with toys( K=1000000 ) Covariance normalized to correlation matrix for iterative bayes with 2 iterations without and with toys( K=1000000 ) Note the large negative correlations with the matrix inversion unfolding as expected in the unregularized scenario. Iterative bayes with 2 iterations is on the contrary very regularized and mostly has very strong positive correlations. Bias Lets recall the definition of the bias. b[\\hat{\\vec{\\mu}}] = E[\\hat{\\vec{\\mu}}] - \\vec{\\mu} Many of the above algorithms do not have an explicit definition for the bias. However, the same approach to pseudo-data generation as for the covariance matrix estimation can be used to estimate the bias of the unfolding framework. The bias for the i -th unfolded bin can be estimated with b_{i} = \\frac{1}{K}\\sum_{k}^{K}(\\hat{\\mu}_{i})_{k} - \\mu_{i} with \\mu_{i} being the i -th bin of a truth distribution chosen by the analyst. MSE and coverage In this section two quantities are discussed that can be used to assess an estimator and fine tune the regularization parameter. MSE As mentioned before, two sources of error of an estimator are the bias and the variance. One could choose an estimator and regularization parameter that minimizes both sources of error. A very common figure of merit is the mean squared error averaged over the bins of the truth histogram. MSE = \\frac{1}{N}\\sum_{i}^{N}(\\mbox{Cov}(\\hat{\\mu}_{i}, \\hat{\\mu}_{i}) + b_{i}^{2}) However, it is possible that bins with a large bin count relative to others in the histogram will dominate. An alternative weighted MSE can be used to avoid this. MSE_{w} = \\frac{1}{N}\\sum_{i}^{N}\\frac{(\\mbox{Cov}(\\hat{\\mu}_{i}, \\hat{\\mu}_{i}) + b_{i}^{2})}{\\hat{\\mu_{i}}^{2}} Coverage When one has an estimate for a truth bin \\hat{\\mu}_{i} and a corresponding variance \\mbox{Cov}(\\hat{\\mu}_{i}, \\hat{\\mu}_{i}) = \\sigma_{\\hat{\\mu}_{i}}^{2} one can construct confidence intervals [\\hat{\\mu}_{i} - Z\\sigma_{\\hat{\\mu}_{i}}, \\hat{\\mu}_{i} + Z\\sigma_{\\hat{\\mu}_{i}}] . Z is a positive integer that determines the size of the confidence interval eg. 1 for a 68.3% confidence interval. The estimator should in the limit of infinite repeated experiments cover the true distribution \\vec{\\mu} 68.3% of the times. Under the assumption that the observed data is Gaussian distributed one can calculate this coverage probability in closed form P_{cov} = \\Phi(\\frac{b_{i}}{\\sigma_{\\hat{\\mu}_{i}}} + Z) - \\Phi(\\frac{b_{i}}{\\sigma_{\\hat{\\mu}_{i}}} - Z) with \\Phi being the Standard Gaussian cumulative distribution function and Z corresponding the size of the constructed confidence interval. The coverage is also connected to regularization. An unbiased estimator( b_{i}=0 ), like the ML estimator, will have a perfect coverage probability of 68.3%. In case bias is introduced by regularizing then the estimators are expected to undercover. It is up to the analyst to decide what amount of undercoverage is acceptable. M. J. Kuusela. Uncertainty quantification in unfolding elementary particle spectra at the large hadron collider. pages 84\u201386, 2016.","title":"Tutorial"},{"location":"tutorial/#motivation-for-unfolding","text":"In any experimental setup the measuring device will have imperfections such as finite resolution, limited acceptance or detection efficiency. These imperfections will result in a measurement that is distorted from nature. Correcting for these distortions is known as unfolding. The general motivation for unfolding is to get results that are not measurement device dependent and can be compared with the results of other experiments. It should be noted that it is not always necessary to unfold. In case one wants to compare a measurement to a theoretical prediction it is often easier to simulate the detector effects on the theoretical prediction. However, for a measurement to retain its value over time and be comparable with possible future theoretical predictions the detector response, in one form or another, should be preserved as well. This is often impractical. Unfolding provides a result which can be directly compared with those of other experiments and theoretical predictions.","title":"Motivation for unfolding"},{"location":"tutorial/#mathematical-formulation","text":"Lets assume we have some measurable quantity, eg. the transverse momentum of a particle, respresented by the random variable x with probability density function g(x) . Each measured value x has a true value for a random variable y with probability density function f(y) . These two pdfs are related by a convolution g(x) = \\int R(x|y) f(y) dy where R(x|y) is the response function. In particle physics, the measured quantities are binned into histograms so we will focus on the discretized version of the problem. The above convolution changes to the linear equation \\vec{\\nu} = R\\vec{\\mu} with \\vec{\\nu} being the N -vector of expectation values for the bins of observed variable x and \\vec{\\mu} being the M -vector of expectation values for the histogram of y . The histogram of the actual observed data is given by the N -vector \\vec{n} . The response matrix R is a N \\times M matrix and has an interpretation of a conditional probability. R_{ij} = P(\\mbox{observed in bin }i | \\mbox{true value in bin }j) = \\frac{P(\\mbox{observed in bin }i\\mbox{ and } \\mbox{true value in bin }j)}{P(\\mbox{true value in bin }j)} The observed data \\vec{n} , their expectation values \\vec{\\nu} , the response matrix R and the expectation values of the true histogram \\vec{\\mu} are finally related by E[\\vec{n}] = \\vec{\\nu} = R\\vec{\\mu}","title":"Mathematical formulation"},{"location":"tutorial/#building-the-response-matrix","text":"The first step of unfolding is building the response matrix. Here we assume that the analyst has chosen an observable to unfold, a Monte Carlo simulation to fill the expected truth distribution \\vec{\\mu} and a detector simulation to fill the expected reconstructed distribution \\vec{\\nu} . The result is a table of events with a true and reconstructed value of the chosen observable. Lets assume we filled a 2D histogram A with the reconstructed values on the x-axis and the true values on the y-axis. Recall that R_{ij} = \\frac{P(\\mbox{observed in bin }i\\mbox{ and } \\mbox{true value in bin }j)}{P(\\mbox{true value in bin }j)} These probabilities can be approximated in the limit of many events by P(\\mbox{observed in bin }i\\mbox{ and } \\mbox{true value in bin }j) = \\frac{\\mbox{Number of reconstructed events in bin }i\\mbox{ and true events in } j}{\\mbox{Total number of events}} = \\frac{A_{ij}}{\\sum_{i}^{N}\\sum_{j}^{M}A_{ij}} and P(\\mbox{true value in bin }j) = \\frac{\\mbox{Number of true events in bin }j}{\\mbox{Total number of events}} = \\frac{\\sum_{i}^{N}A_{ij}}{\\sum_{i}^{N}\\sum_{j}^{M}A_{ij}} The response matrix element can therefore be approximated by R_{ij} = \\frac{A_{ij}}{\\sum_{i}^{N}A_{ij}} One should note that this approximation will hold for diagonal matrix elements and the ones close to the diagonal i.e. the elements corresponding to heavily populated bins. One can change the binning to increase the bin counts or apply some sort of smoothing to ensure the validity of the response matrix.","title":"Building the response matrix"},{"location":"tutorial/#efficiency","text":"In the above we assumed that for every true value there is a corresponding recontructed value. However, in reality the detector will have an efficiency resulting in a fraction of the true events not being reconstructed. Because one can only use events with both a true and reconstructed value to fill the histogram A the sum \\sum_{i}^{N}A_{ij} will no longer be equal to the total number of generated events. Instead one needs to fill a 1D histogram \\vec{\\mu} with all the true values and use that to calculate the response matrix. R_{ij} = \\frac{A_{ij}}{\\mu_{j}} The efficiency i.e. the number of events from truth bin j that are reconstructed in any bin is then \\epsilon_{j} = \\sum_{i}^{N}R_{ij}","title":"Efficiency"},{"location":"tutorial/#code-example","text":"Lets start with some code that simulates event generation. In this example we use RooFit classes to sample values from a falling distribution. // A variable for the truth distribution. RooRealVar xtrue(\"xtrue\",\"xtrue\",-5,5); // A parameter of the falling distribution defining the shape. RooRealVar lambda(\"lambda\",\"lambda\",-0.00035); // The falling distribution. RooExponential truthPDF(\"truthPDF\",\"truthPDF\", xtrue, lambda); Define some simple variable dependent smearing and efficiency function. Double_t Smear(Double_t xtrue){ // Simulate efficiency. Double_t xeff= 0.3 + (1.0-0.3)/20*(xt+10.0); Double_t x= gRandom- Rndm(); if (x xeff) return -1; // Simulate smearing. Double_t xsmear= gRandom- Gaus(0,0.05*xtrue); return xt+xsmear; } Define the binning of the truth and reconstructed distribution. Double_t reco_binning[26] = {0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,20,25,30,35,40,50,60}; Double_t truth_binning[13] = {0,2,4,6,8,10,12,14,18,25,35,45,60}; TH1D* truthHist = new TH1D(\"truthHist\",\"truthHist\",12,truth_binning); TH1D* recoHist = new TH1D(\"recoHist\",\"recoHist\",25,reco_binning); TH2D* responseHist = new TH2D(\"response\",\"response\", 25, reco_binning, 12, truth_binning); Define the histograms for some data to unfold and a truth distribution to compare the unfolding results with. TH1D* truthTestHist = new TH1D(\"truthTestHist\",\"truthTestHist\",12,truth_binning); TH1D* dataHist = new TH1D(\"dataHist\",\"dataHist\",25,reco_binning); Loop over the generated events, smear them and use them to fill the histograms. // Realistically, there are more MC events than data events available. Int_t n_MC_events = 500000; Int_t n_data_events = 10000; // Calculate the lumi weight with which the MC will be scaled down. Double_t lumiScale = (double)n_data_events/n_MC_events; // Generate the events; RooDataSet* MCEvents = truthPDF.generate(xtrue,n_MC_events); RooDataSet* dataEvents = truthPDF.generate(xtrue,n_data_events); // Loop over both datasets and fill the histograms. for (int i = 0; i n_MC_events; i++){ Double_t xtrue_val = ((RooRealVar*)(MCEvents- get(i))- find(xtrue.GetName()))- getVal(); Double_t xreco_val = Smear(xtrue_val); truthHist- Fill(xtrue_val, lumiScale); if (xreco_val 0){ recoHist- Fill(xreco_val, lumiScale); responseHist- Fill(xreco_val, xtrue_val, lumiScale); } } for (int i = 0; i n_data_events; i++){ Double_t xtrue_val = ((RooRealVar*)(dataEvents- get(i))- find(xtrue.GetName()))- getVal(); Double_t xreco_val = Smear(xtrue_val); truthTestHist- Fill(xtrue_val); if (xreco_val 0){ dataHist- Fill(xreco_val); } } Create a factory object that will build the response matrix and book keep all of the histograms. RooUnfoldSpec spec(\"unfold\",\"unfold\",truthHist,\"obs_truth\",recoHist,\"obs_reco\",responseHist,NULL,dataHist,false,0.0005); Input distributions Unnormalized response matrix","title":"Code Example"},{"location":"tutorial/#matrix-inversion-and-bin-by-bin","text":"In this section we will discuss two unfolding methods i.e. two approaches for building estimators for the true distribution \\vec{\\mu} . These methods can be used in certain scenarios but often will show to give unsatisfactory results.","title":"Matrix inversion and bin-by-bin"},{"location":"tutorial/#matrix-inversion","text":"Suppose the response matrix can be inverted. In that case, one can write \\vec{\\mu} = R^{-1}\\vec{\\nu} A reasonable choice of estimators for \\vec{\\mu} would then be \\hat{\\vec{\\mu}} = R^{-1}\\vec{n} One can show that this is equivalent to a maximum likelihood(ML) solution under the assumption that the elements of the data vector \\vec{n} are independent and Poisson distributed around mean \\vec{\\nu} . In case the off-diagonal elements of the response matrix are large the estimator \\hat{\\vec{\\mu}} will have large variances and strong negative correlations. Show response matrix, truth, reconstructed, data and largely fluctuating estimator of the truth histogram. The small fluctuations in \\vec{n} are \"seen by R^{-1} \" as a fine structure that is enlarged in the estimators. One should note though that the ML/matrix inversion estimators are unbiased and have the smallest possible variance. We will show later that in order to reduce the variance one needs to introduce some bias by regularization.","title":"Matrix inversion"},{"location":"tutorial/#bin-by-bin","text":"An alternative method with much smaller variances is applying bin-by-bin correction factors. \\hat{\\mu}_{i} = C_{i}n_{i} with C_{i} = A_{ii} This method is effectively the same as applying the matrix inversion method with all the off-diagonal elements set to zero. This will reduce the variance substantially but also introduce some bias. Both the matrix inversion and the bin-by-bin methods can be reasonable solutions in case of very small off-diagonal response matrix elements. However, often one needs to apply some form of regularization that constrains \\vec{\\mu} -space in which solutions are deemed reasonable.","title":"Bin-by-bin"},{"location":"tutorial/#code-example_1","text":"Lets use the generated data to unfold with both matrix inversion and bin-by-bin. // Instantiate a RooUnfoldFunc RooUnfoldFunc* unfoldFuncInvert = (RooUnfoldFunc*)spec.makeFunc(RooUnfolding::kInvert); RooUnfoldFunc* unfoldFuncBBB = (RooUnfoldFunc*)spec.makeFunc(RooUnfolding::kBinByBin); // Instantiate a RooUnfold object with RooFitHist as template type. RooUnfoldT RooUnfolding::RooFitHist,RooUnfolding::RooFitHist * unfoldInvert = const_cast RooUnfoldT RooUnfolding::RooFitHist,RooUnfolding::RooFitHist * (unfoldFuncInvert- unfolding()); RooUnfoldT RooUnfolding::RooFitHist,RooUnfolding::RooFitHist * unfoldBBB = const_cast RooUnfoldT RooUnfolding::RooFitHist,RooUnfolding::RooFitHist * (unfoldFuncBBB- unfolding()); // Unfold and get the unfolded distribution and errors. TVectorD invertUnfolded = unfoldInvert- Vunfold(); TVectorD invertErrors = unfoldInvert- EunfoldV(RooUnfolding::kErrors); TVectorD BBBUnfolded = unfoldBBB- Vunfold(); TVectorD BBBErrors = unfoldBBB- EunfoldV(RooUnfolding::kErrors); Distribution unfolded with matrix inversion Distribution unfolded with Bin-by-bin","title":"Code Example"},{"location":"tutorial/#regularization-and-bias-variance-trade-off","text":"Recall that the observed data \\vec{n} is independent and Poisson distributed around \\vec{\\nu} . We can therefore construct the following log-likelihood. \\log L(\\vec{\\mu}) = \\sum_{i}^{N}(n_{i}\\log \\nu_{i} - \\nu_{i}) \\mbox{ with } \\nu_{i} = \\sum_{j}^{M}R_{ij}\\mu_{j} The solution that maximizes this likelihood is \\vec{n} = \\vec{\\nu} i.e. the matrix inversion solution. This is an example of overfitting and can be avoided by introducing some constraint on the likelihood maximization in the form of a regularization function. \\phi(\\vec{\\mu}) = \\log L(\\vec{\\mu}) + \\tau S(\\vec{\\mu}) Here the regularization function introduces some form of prior knowledge on what the solution should look like i.e. it introduces bias but reduces the variance of the estimator \\hat{\\vec{\\mu}} . This is also known as the bias-variance trade-off. Both variance and bias are sources of error on the predictive model that is the truth estimator \\hat{\\vec{\\mu}} . The regularization parameter \\tau determines where this bias-variance trade-off will lie. We will discuss later how to choose the regularization paremeter. The choice of S(\\vec{\\mu}) depends on the problem but often is chosen to impose some amount of smoothness on the estimator \\vec{\\mu} eg. with Tikhonov or entropy-based regularization. In the following section we will discuss some unfolding algorithms and their form of regularization that are commonly used in particle physics.","title":"Regularization and bias-variance trade-off"},{"location":"tutorial/#tikhonov-regularization","text":"A very common choice of S(\\vec{\\mu}) is one based on the derivatives of the true distribution. Tikhonov regularization constructs a regularization function by taking the square of the k -th derivative of the true distribution. For k=2 and with finite differences the regularization function is S(\\vec{\\mu}) = - \\sum_{i=1}^{M-2}(-\\mu_{i} + 2\\mu_{i+1} - \\mu_{i+2})^{2} This function quantifies the amount of curvature the true distribution has. Distributions such as the ML solution have very large second derivatives and are therefore penalized heavily. A. N. Tikhonov and V. Y. Arsenin. Solutions of Ill-posed problems. W.H. Winston, 1977.","title":"Tikhonov regularization"},{"location":"tutorial/#code-example_2","text":"RooUnfold has two Tikhonov regularized unfolding implementations: RooUnfoldSVD and RooUnfoldTUnfold. RooUnfoldSVD will be discussed seperately in an upcoming section. TUnfold implements a Gaussian likelihood and calculates the unfolded distribution with a closed form. // Instantiate a RooUnfoldFunc. Pass the regularization parameter tau=0.001. RooUnfoldFunc* unfoldFuncTUnfold = (RooUnfoldFunc*)spec.makeFunc(RooUnfolding::kTUnfold,0.001); // Instantiate a RooUnfold object with RooFitHist as template type. RooUnfoldT RooUnfolding::RooFitHist,RooUnfolding::RooFitHist * unfoldTUnfold = const_cast RooUnfoldT RooUnfolding::RooFitHist,RooUnfolding::RooFitHist * (unfoldFuncTUnfold- unfolding()); // Unfold and get the unfolded distribution and errors. TVectorD tunfoldUnfolded = unfoldTUnfold- Vunfold(); TVectorD tunfoldErrors = unfoldTUnfold- EunfoldV(RooUnfolding::kErrors); Gaussian likelihood and Tikhonov regularization with \\tau=10 and \\tau=0.001 Gaussian likelihood and Tikhonov regularization with \\tau=0.0005 and \\tau=0.000001","title":"Code Example"},{"location":"tutorial/#richardson-lucy-iterative-bayes","text":"The Richardson-Lucy algorithm, or iterative bayes unfolding as described by D\u2019Agostini in, is an unfolding procedure that updates the estimator \\vec{\\mu} with each iteration. The estimator is defined as \\hat{\\mu}_{j}^{r+1} = \\hat{\\mu}_{j}^{r}\\sum_{i}^{N}\\frac{R_{ij}n_{i}}{\\sum_{k}^{M}R_{ik}\\hat{\\mu}_{j}^{r}} for the j -th bin and r+1 iterations. The analyst needs to supply an initial truth distribution \\hat{\\vec{\\mu}}^{0} . The number of iterations determines the amount of regularization where few iterations will result in an estimator with small variance but will be biased towards this initial truth distribution. The bias-variance balance can be tipped in the other direction by increasing the number of iterations. In the limit of many iterations it is emperically shown that the estimator will converge to the ML estimator. W. H. Richardson. Bayesian-based iterative method of image restoration. J. Opt. Soc. Am., 62(1):55\u201359, Jan 1972. L. B. Lucy. An iterative technique for the rectification of observed distributions. 79:745, Jun 1974. G. D\u2019Agostini. A multidimensional unfolding method based on bayes theorem. 1995. Nucl. Instrum. Meth. A 362 (1995) 487.","title":"Richardson-Lucy (Iterative Bayes)"},{"location":"tutorial/#code-example_3","text":"// Instantiate a RooUnfoldFunc. Pass the regularization parameter r=20. RooUnfoldFunc* unfoldFuncBayes = (RooUnfoldFunc*)spec.makeFunc(RooUnfolding::kBayes,20); // Instantiate a RooUnfold object with RooFitHist as template type. RooUnfoldT RooUnfolding::RooFitHist,RooUnfolding::RooFitHist * unfoldBayes = const_cast RooUnfoldT RooUnfolding::RooFitHist,RooUnfolding::RooFitHist * (unfoldFuncBayes- unfolding()); // Unfold and get the unfolded distribution and errors. TVectorD bayesUnfolded = unfoldBayes- Vunfold(); TVectorD bayesErrors = unfoldBayes- EunfoldV(RooUnfolding::kErrors); Iterative bayes with r=1 and r=20 Iterative bayes with r=100 and r=2000","title":"Code Example"},{"location":"tutorial/#singular-value-decomposition-svd","text":"Singular Value Decomposition is an unfolding method that constructs a likelihood with the data \\vec{n} Gaussian distributed. The mean is taken to be the expected bin count \\vec{\\nu} and a measured covariance V_{ij} . The regularization function is the discretized Tikhonov second derivative regularization. The constrained log-likelihood can be written down in matrix form as \\phi(\\vec{\\mu}) = -\\frac{1}{2}(R\\vec{\\mu} - \\vec{n})V^{-1}(R\\vec{\\mu} - \\vec{n})^{T} + \\tau (C\\vec{\\mu})^{T} (C\\vec{\\mu}) SVD unfolding sets this expression to zero and solves for \\vec{\\mu} with the use of singular value decomposition. It also uses singular value decomposition to determine which of the data bins contribute the most to large fluctuations in the unfolded distribution and use that information to choose \\tau . The user actually choses an integer k between 0 and the number of truth bins N which then is translated into a value for \\tau that supresses the N-k data bins that contribute the most to the unwanted large fluctuations in the unfolded distribution. So k=0 will result in a heavily regularized distribution and k=N will return the ML solution. Hocker and Kartvelishvili. Svd-based unfolding: implementation and experience. PHYSTAT 2011, 2011. K. Tackmann. Svd approach to data unfolding. 1996. Nucl. Instrum. Meth. A 372 (1996) 469.","title":"Singular Value Decomposition (SVD)"},{"location":"tutorial/#code-example_4","text":"// Instantiate a RooUnfoldFunc. Pass the regularization parameter k=8. RooUnfoldFunc* unfoldFuncSvd = (RooUnfoldFunc*)spec.makeFunc(RooUnfolding::kSVD,8); // Instantiate a RooUnfold object with RooFitHist as template type. RooUnfoldT RooUnfolding::RooFitHist,RooUnfolding::RooFitHist * unfoldSvd = const_cast RooUnfoldT RooUnfolding::RooFitHist,RooUnfolding::RooFitHist * (unfoldFuncSvd- unfolding()); // Unfold and get the unfolded distribution and errors. TVectorD svdUnfolded = unfoldSvd- Vunfold(); TVectorD svdErrors = unfoldSvd- EunfoldV(RooUnfolding::kErrors); SVD with k=2 and k=8 SVD with k=10 and k=12","title":"Code Example"},{"location":"tutorial/#iterative-dynamically-stabilized-ids","text":"Iterative Dynamically Stabilized unfolding is an unfolding procedure that starts from an initial unfolded distribution very biased towards a normalized truth distribution. It then tries to improve the result by improving the response matrix with each iteration. The estimators are defined as \\hat{\\mu_{i}} = \\mu_{i}^{MC}C + \\Delta\\beta_{i}^{true} + \\sum_{j}^{M}\\big(f(\\Delta n_{i}, \\sigma_{\\Delta n_{i}}, \\lambda)R_{ji}\\Delta n_{i} - (1 - f(\\Delta n_{i}, \\sigma_{\\Delta n_{i}}, \\lambda))\\Delta n_{i}\\delta_{ji}\\big) with \\mu_{i}^{MC} being some initial truth distribution, C a normalization constant that corrects for differences in MC and data, \\Delta\\beta_{i}^{true} the expected number of truth events associated with fluctations in background subtraction, \\Delta n_{i} = n_{i} - \\Delta\\beta_{i}^{reco} - C\\nu_{i} , \\sigma_{\\Delta n_{i}} the uncertainty on the measured data n_{i} , R_{ji} the response matrix, f(\\Delta n_{i}, \\sigma_{\\Delta n_{i}}, \\lambda) a number between 0 and 1 and \\lambda that regulates this fraction. This is used for an initial unfolded distribution and then improved by changing the response matrix R_{ij} with each iteration. B. Malaescu. An iterative, dynamically stabilized method of data unfolding. arXiv:0907.3791.","title":"Iterative Dynamically Stabilized (IDS)"},{"location":"tutorial/#code-example_5","text":"// Instantiate a RooUnfoldFunc RooUnfoldFunc* unfoldFuncIds = (RooUnfoldFunc*)spec.makeFunc(RooUnfolding::kIDS,10); // Instantiate a RooUnfold object with RooFitHist as template type. RooUnfoldT RooUnfolding::RooFitHist,RooUnfolding::RooFitHist * unfoldIds = const_cast RooUnfoldT RooUnfolding::RooFitHist,RooUnfolding::RooFitHist * (unfoldFuncIds- unfolding()); // Unfold and get the unfolded distribution and errors. TVectorD idsUnfolded = unfoldIds- Vunfold(); TVectorD idsErrors = unfoldIds- EunfoldV(RooUnfolding::kErrors); IDS with \\lambda=2 and \\lambda=20 IDS with \\lambda=100 and \\lambda=200","title":"Code Example"},{"location":"tutorial/#gaussian-processes-gp","text":"Gaussian Processes uses Bayesian regression to define an estimator as the mode of the posterior distribution. The posterior distribution is constructed with Bayes' theorem from a Gaussian likelihood and a Gaussian Process(GP) prior. Regularisation is introduced via the kernel function of the GP. Depending on the prior knowledge one has on the truth distribution different choices of kernels are possible. The amount of regularisation is determined by the parameters of the kernel which can be varied until unfolding results show satisfactory. It is also possible to choose these parameters based on the maximization of the likelihood marginalized over the truth distribution. The current implementation of RooUnfold has two kernel functions available: Radial kernel (Parameters: (A, l) ) k(x, x') = A \\mbox{exp}\\Bigg( \\frac{-(x - x')^{2}}{2l^{2}}\\Bigg) Gibbs kernel (Parameters: (A,b,c) ) k(x, x') = A \\sqrt{\\frac{2l(x)l(x')}{l^{2}(x) + l^{2}(x')}}\\mbox{exp}\\Bigg( \\frac{-(x - x')^{2}}{l^{2}(x) - l^{2}(x')}\\Bigg) with l(x) = bx + c and x and x' denoting two bin centers. A. Bozson, G. Cowan, and F. Spano. Unfolding with Gaussian Processes. 11 2018.","title":"Gaussian Processes (GP)"},{"location":"tutorial/#code-example_6","text":"// Instantiate a RooUnfoldFunc. The regularization parameter determines the kernel. // 1 = Radial kernel and 2 = Gibbs kernel. RooUnfoldFunc* unfoldFuncGP = (RooUnfoldFunc*)spec.makeFunc(RooUnfolding::kGP,2); // Instantiate a RooUnfold object with RooFitHist as template type. RooUnfoldT RooUnfolding::RooFitHist,RooUnfolding::RooFitHist * unfoldGP = const_cast RooUnfoldT RooUnfolding::RooFitHist,RooUnfolding::RooFitHist * (unfoldFuncGP- unfolding()); // The unfolding automatically maximizes a marginalized likelihood to find // the optimal kernel parameters. However, it is possible to tune the kernel parameters // and therefore the regularization by hand. Add the parameters in the order of (A, L) and // (A, b, c). std::vector double kernelParm; kernelParm.push_back(22); kernelParm.push_back(0.00001); kernelParm.push_back(9.9); ((RooUnfoldGPT RooUnfolding::RooFitHist,RooUnfolding::RooFitHist *)unfold)- SetKernelParm(kernelParm); // Unfold and get the unfolded distribution and errors. TVectorD gpUnfolded = unfoldGP- Vunfold(); TVectorD gpErrors = unfoldGP- EunfoldV(RooUnfolding::kErrors); GP with a Gibbs kernel with maximized marginal likelihood optimized kernel parameters and the above chosen kernel parameters.","title":"Code Example"},{"location":"tutorial/#covariance-matrix-and-bias","text":"As mentioned before, the estimators \\hat{\\vec{\\mu}} have two sources of error, variance and bias. These are the quantities that the analyst should first look at when assessing their unfolding algorithm. Also, one will see in upcoming sections that these quantities and derivations thereof can be used to compare unfolding algorithms and tune the regularization. Additionally, one needs the covariance matrix if one wants to use the unfolded distribution in a fit.","title":"Covariance matrix and bias"},{"location":"tutorial/#covariance","text":"Lets recall the definition of the covariance matrix. \\mbox{Cov}(\\hat{\\mu}_{i}, \\hat{\\mu}_{j}) = E[(\\hat{\\mu}_{i} - E[\\hat{\\mu}_{i}])(\\hat{\\mu}_{j} - E[\\hat{\\mu}_{j}])] All of the above mentioned algorithms have their own approach for estimating the covariance matrix. However, it is also possible to estimate the covariance matrix with the use of toy data. Generating toy data or \"throwing toys\" is sampling pseudo-data \\vec{n}_{toy} from probability distributions of the observed data \\vec{n} . In particle physics, a very common choice is Poisson distributions with mean \\vec{\\nu} . These pseudo-data vectors can then be unfolded with the chosen unfolding algorithm and regularization parameter. In the limit of many toys these unfolded distributions can be used to calculate the covariance with the sample covariance \\mbox{C}(\\hat{\\mu}_{i}, \\hat{\\mu}_{j}) = \\frac{1}{K-1}\\sum_{k}^{K}\\Big(\\big((\\hat{\\mu}_{i})_{k} - \\frac{1}{K}\\sum_{k}^{K}(\\hat{\\mu}_{i})_{k}\\big)\\big((\\hat{\\mu}_{j})_{k} - \\frac{1}{K}\\sum_{k}^{K}(\\hat{\\mu}_{j})_{k}\\big)\\Big) with (\\hat{\\mu}_{i})_{k} being the i -th unfolded bin of the k -th toy for K toy datasets.","title":"Covariance"},{"location":"tutorial/#code-example_7","text":"// Calculate the covariance matrix with the unfolding algorithm specific method. TMatrixD cov = unfold- Eunfold(RooUnfolding::kCovariance); // Set the number of toys. unfold- SetNToys(1000000); // Calculate the covariance matrix with toys. TMatrixD covtoys = unfold- Eunfold(RooUnfolding::kCovToys); Covariance normalized to correlation matrix for matrix inversion without and with toys( K=1000000 ) Covariance normalized to correlation matrix for iterative bayes with 2 iterations without and with toys( K=1000000 ) Note the large negative correlations with the matrix inversion unfolding as expected in the unregularized scenario. Iterative bayes with 2 iterations is on the contrary very regularized and mostly has very strong positive correlations.","title":"Code Example"},{"location":"tutorial/#bias","text":"Lets recall the definition of the bias. b[\\hat{\\vec{\\mu}}] = E[\\hat{\\vec{\\mu}}] - \\vec{\\mu} Many of the above algorithms do not have an explicit definition for the bias. However, the same approach to pseudo-data generation as for the covariance matrix estimation can be used to estimate the bias of the unfolding framework. The bias for the i -th unfolded bin can be estimated with b_{i} = \\frac{1}{K}\\sum_{k}^{K}(\\hat{\\mu}_{i})_{k} - \\mu_{i} with \\mu_{i} being the i -th bin of a truth distribution chosen by the analyst.","title":"Bias"},{"location":"tutorial/#mse-and-coverage","text":"In this section two quantities are discussed that can be used to assess an estimator and fine tune the regularization parameter.","title":"MSE and coverage"},{"location":"tutorial/#mse","text":"As mentioned before, two sources of error of an estimator are the bias and the variance. One could choose an estimator and regularization parameter that minimizes both sources of error. A very common figure of merit is the mean squared error averaged over the bins of the truth histogram. MSE = \\frac{1}{N}\\sum_{i}^{N}(\\mbox{Cov}(\\hat{\\mu}_{i}, \\hat{\\mu}_{i}) + b_{i}^{2}) However, it is possible that bins with a large bin count relative to others in the histogram will dominate. An alternative weighted MSE can be used to avoid this. MSE_{w} = \\frac{1}{N}\\sum_{i}^{N}\\frac{(\\mbox{Cov}(\\hat{\\mu}_{i}, \\hat{\\mu}_{i}) + b_{i}^{2})}{\\hat{\\mu_{i}}^{2}}","title":"MSE"},{"location":"tutorial/#coverage","text":"When one has an estimate for a truth bin \\hat{\\mu}_{i} and a corresponding variance \\mbox{Cov}(\\hat{\\mu}_{i}, \\hat{\\mu}_{i}) = \\sigma_{\\hat{\\mu}_{i}}^{2} one can construct confidence intervals [\\hat{\\mu}_{i} - Z\\sigma_{\\hat{\\mu}_{i}}, \\hat{\\mu}_{i} + Z\\sigma_{\\hat{\\mu}_{i}}] . Z is a positive integer that determines the size of the confidence interval eg. 1 for a 68.3% confidence interval. The estimator should in the limit of infinite repeated experiments cover the true distribution \\vec{\\mu} 68.3% of the times. Under the assumption that the observed data is Gaussian distributed one can calculate this coverage probability in closed form P_{cov} = \\Phi(\\frac{b_{i}}{\\sigma_{\\hat{\\mu}_{i}}} + Z) - \\Phi(\\frac{b_{i}}{\\sigma_{\\hat{\\mu}_{i}}} - Z) with \\Phi being the Standard Gaussian cumulative distribution function and Z corresponding the size of the constructed confidence interval. The coverage is also connected to regularization. An unbiased estimator( b_{i}=0 ), like the ML estimator, will have a perfect coverage probability of 68.3%. In case bias is introduced by regularizing then the estimators are expected to undercover. It is up to the analyst to decide what amount of undercoverage is acceptable. M. J. Kuusela. Uncertainty quantification in unfolding elementary particle spectra at the large hadron collider. pages 84\u201386, 2016.","title":"Coverage"}]}